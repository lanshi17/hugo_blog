---
# æ ¸å¿ƒå…ƒæ•°æ®
author: lanshi
date: "2025-09-13T12:47:02+08:00"
lastmod:
title: ç«åŠ›å‘ç”µæ•ˆç‡æ•°æ®ç‰¹å¾å·¥ç¨‹

# å†…å®¹æ§åˆ¶
draft: false
showToc: true
tocOpen: false
showFullContent: true
summary: æœ¬æ–‡ç³»ç»Ÿæ¢³ç†äº†ä»æ•°æ®åŠ è½½åˆ°ç‰¹å¾å·¥ç¨‹æ ¸å¿ƒæ­¥éª¤çš„æŠ€æœ¯æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å½’ä¸€åŒ–ã€æ­£æ€åŒ–ã€å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤šé‡å…±çº¿æ€§å¤„ç†ã€‚ç»“åˆæŠ€æœ¯åŸç†è¯´æ˜ã€ä»£ç è§£æå’Œå¯è§†åŒ–åˆ†æï¼Œé€‚ç”¨äºæ•°æ®ç§‘å­¦/æœºå™¨å­¦ä¹ åˆå­¦è€…æˆ–ä¸­çº§ç”¨æˆ·é˜…è¯»ã€‚

# å†…å®¹åˆ†ç±»
series:
tags: ["æ•°æ®é¢„å¤„ç†", "Python", "Pandas", "Scikit-learn", "Seaborn", "Matplotlib", "å½’ä¸€åŒ–", "æ­£æ€åŒ–", "å¼‚å¸¸å€¼æ£€æµ‹", "å¤šé‡å…±çº¿æ€§"]
categories: ["æ•°æ®ç§‘å­¦", "æœºå™¨å­¦ä¹ "]

# SEOä¼˜åŒ–
description: æœ¬æ–‡ç³»ç»Ÿæ¢³ç†äº†ä»æ•°æ®åŠ è½½åˆ°ç‰¹å¾å·¥ç¨‹æ ¸å¿ƒæ­¥éª¤çš„æŠ€æœ¯æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å½’ä¸€åŒ–ã€æ­£æ€åŒ–ã€å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤šé‡å…±çº¿æ€§å¤„ç†ã€‚ç»“åˆæŠ€æœ¯åŸç†è¯´æ˜ã€ä»£ç è§£æå’Œå¯è§†åŒ–åˆ†æï¼Œé€‚ç”¨äºæ•°æ®ç§‘å­¦/æœºå™¨å­¦ä¹ åˆå­¦è€…æˆ–ä¸­çº§ç”¨æˆ·é˜…è¯»ã€‚
keywords: ["æ•°æ®é¢„å¤„ç†", "Python", "Pandas", "Scikit-learn", "Seaborn", "Matplotlib", "å½’ä¸€åŒ–", "æ­£æ€åŒ–", "å¼‚å¸¸å€¼æ£€æµ‹", "å¤šé‡å…±çº¿æ€§", "Box-Cox", "Yeo-Johnson", "VIF", "PCA", "Ridge", "Lasso", "SVR", "XGBoost"]

# ä¸»é¢˜é›†æˆ
math: true
comment: true
hiddenFromSearch: false
hiddenFromHomePage: false

# è§†è§‰é…ç½®
cover:
  image: "data-preprocessing-cover.png"
  alt: "æ•°æ®é¢„å¤„ç†å…¨æµç¨‹å®æˆ˜å°é¢"
  caption: "æ•°æ®é¢„å¤„ç†å…¨æµç¨‹å®æˆ˜"
  relative: true

# ç‰ˆæƒå£°æ˜
copyright: true
---

---


> åŸæ–‡æ¥è‡ª Kaggle æŸç«èµ›æ•°æ®é›†ï¼ˆ`processed_zhengqi_data.csv`ï¼‰ï¼Œæœ¬ç¬”è®°ç³»ç»Ÿæ¢³ç†äº†ä»æ•°æ®åŠ è½½åˆ°ç‰¹å¾å·¥ç¨‹æ ¸å¿ƒæ­¥éª¤çš„æŠ€æœ¯æµç¨‹ã€‚
> æŠ€æœ¯æ ˆï¼šPython + Pandas + Scikit-learn + Seaborn + Matplotlib
> é€‚ç”¨äººç¾¤ï¼šæ•°æ®ç§‘å­¦å®¶ã€æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆã€ç®—æ³•å­¦ä¹ è€…

---

## ğŸ” ä¸€ã€å¯¼å…¥æ‰€éœ€åº“

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn import preprocessing
from sklearn.preprocessing import PowerTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.svm import SVR
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings("ignore")
```

> âœ… **è¯´æ˜**ï¼š
> - `seaborn` å’Œ `matplotlib` ç”¨äºå¯è§†åŒ–ï¼›
> - `scipy.stats` æä¾›ç»Ÿè®¡æ£€éªŒåŠŸèƒ½ï¼ˆå¦‚ååº¦ã€Q-Qå›¾ï¼‰ï¼›
> - `PowerTransformer` å®ç° Box-Cox / Yeo-Johnson å˜æ¢ï¼›
> - æ‰€æœ‰æ¨¡å‹ç”¨äºå¼‚å¸¸å€¼æ£€æµ‹ã€‚

---

## ğŸ“¥ äºŒã€æ•°æ®åŠ è½½ä¸åˆæ­¥è§‚å¯Ÿ

### 1. åŠ è½½æ•°æ®

```python
all_data = pd.read_csv('./data/processed_zhengqi_data.csv')
display(all_data.head())
```

- æ•°æ®åŒ…å«å¤šä¸ªç‰¹å¾åˆ— `V0`, `V1`, ..., `V28`ï¼Œæ ‡ç­¾åˆ— `label`ï¼ˆtrain/testï¼‰ï¼Œç›®æ ‡åˆ— `target`ã€‚
- æˆ‘ä»¬å°†ä½¿ç”¨ `train` æ•°æ®è®­ç»ƒæ¨¡å‹ï¼›`test` ç”¨äºé¢„æµ‹ã€‚

### 2. åˆ†ç¦»è®­ç»ƒé›†

```python
cond = all_data['label'] == 'train'
train_data = all_data[cond].copy()
train_data.drop(columns=['label'], inplace=True)
```

> ğŸ’¡ æç¤ºï¼š`copy()` é¿å…è®¾ç½®å‰¯æœ¬è­¦å‘Šï¼Œæ˜¯è‰¯å¥½å®è·µã€‚

---

## ğŸ“ ä¸‰ã€æ•°æ®å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

å½’ä¸€åŒ–ç›®æ ‡æ˜¯å°†æ‰€æœ‰ç‰¹å¾ç¼©æ”¾åˆ° `[0,1]` åŒºé—´ï¼Œé¿å…æŸäº›ç‰¹å¾å› é‡çº§è¿‡å¤§è€Œä¸»å¯¼æ¨¡å‹è®­ç»ƒã€‚

### æ–¹æ³•ä¸€ï¼šæœ€å°-æœ€å¤§å½’ä¸€åŒ–ï¼ˆMin-Max Scalingï¼‰

#### è‡ªå®šä¹‰å‡½æ•°å®ç°ï¼š

```python
def norm_min_max(col):
    return (col - col.min()) / (col.max() - col.min())
```

#### ä½¿ç”¨ `MinMaxScaler` å®˜æ–¹å·¥å…·ï¼š

```python
min_max_scaler = preprocessing.MinMaxScaler()
all_data_normed = min_max_scaler.fit_transform(all_data[columns])
all_data_normed = pd.DataFrame(all_data_normed, columns=columns)
```

âœ… **ä¼˜ç‚¹**ï¼šç®€å•ç›´è§‚ï¼Œä¿ç•™åŸå§‹åˆ†å¸ƒå½¢æ€ã€‚

âŒ **æ³¨æ„**ï¼šå¯¹ç¦»ç¾¤ç‚¹æ•æ„Ÿï¼Œè‹¥å­˜åœ¨æå€¼å¯èƒ½å¯¼è‡´å‹ç¼©è¿‡åº¦ã€‚

---

## ğŸŒªï¸ å››ã€æ•°æ®æ­£æ€åŒ–ï¼ˆNormalizing Skewed Dataï¼‰

è®¸å¤šæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å›å½’ã€SVMï¼‰å‡è®¾è¾“å…¥æ•°æ®æœä»æ­£æ€åˆ†å¸ƒã€‚å½“ç‰¹å¾å‘ˆåæ–œæ—¶ï¼Œéœ€è¿›è¡Œå˜æ¢ã€‚

### 4.1 åˆ†æ V0 çš„åˆ†å¸ƒç‰¹æ€§

```python
plt.figure(figsize=(12,4))

# å­å›¾1: ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿
sns.histplot(x=train_data['V0'], kde=True, stat="density", ax=plt.subplot(1,3,1))
x = np.linspace(min(train_data['V0']), max(train_data['V0']), 100)
plt.plot(x, stats.norm.pdf(x, *stats.norm.fit(train_data['V0'])), 'r')

# å­å›¾2: Q-Q å›¾ + ååº¦å€¼
stats.probplot(train_data['V0'], plot=plt.subplot(1,3,2))
plt.title(f'skew={stats.skew(train_data["V0"]):.4f}')

# å­å›¾3: ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æ•£ç‚¹å›¾
plt.scatter(train_data['V0'], train_data['target'], s=5, alpha=0.5)
plt.title(f'corr={np.corrcoef(train_data["V0"], train_data["target"])[0][1]:.2f}')
```

ğŸ“Œ **è§‚å¯Ÿç»“æœ**ï¼š
- V0 æ˜æ˜¾å³åï¼ˆskew > 0ï¼‰
- ä¸ target å…³ç³»è¾ƒå¼±ï¼ˆcorr â‰ˆ 0.09ï¼‰
- éæ­£æ€åˆ†å¸ƒ â†’ äºŸéœ€è½¬æ¢

### 4.2 Yeo-Johnson å˜æ¢ â€”â€” å¼ºåŠ›æ­£æ€åŒ–æ–¹æ¡ˆ

Box-Cox å˜æ¢ä»…é€‚ç”¨äºæ­£å€¼ã€‚**Yeo-Johnson** æ˜¯å…¶æ¨å¹¿ç‰ˆï¼Œæ”¯æŒè´Ÿå€¼ä¸é›¶ã€‚

```python
pt = PowerTransformer(method='yeo-johnson', standardize=True)
all_data[columns] = pt.fit_transform(all_data[columns])
```

> âœ… `standardize=True` åŒæ—¶åšæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰
> âœ… æ”¯æŒä»»æ„å®æ•°èŒƒå›´

ğŸ‘‰ **ç»“è®º**ï¼šYeo-Johnson èƒ½æ˜¾è‘—æ”¹å–„æ•°æ®åˆ†å¸ƒï¼Œæå‡åç»­å»ºæ¨¡æ€§èƒ½ã€‚

ğŸ§ª **éªŒè¯å»ºè®®**ï¼šæ£€æŸ¥å˜æ¢å‰åå„ç‰¹å¾çš„ `skewness` å’Œ `Kurtosis`ã€‚

---

## ğŸ§© äº”ã€å°è£…å¸¸ç”¨æ•°æ®å¤„ç†å‡½æ•°

ä¸ºæé«˜å¤ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œæˆ‘ä»¬å°è£…æ ¸å¿ƒæ“ä½œå‡½æ•°ï¼š

```python
def get_train_data():
    train_data = all_data[all_data["label"] == "train"]
    X = train_data.drop(["target", "label"], axis=1)
    y = train_data["target"]
    return X, y

def split_train_data(test_size=0.2):
    X, y = get_train_data()
    return train_test_split(X, y, test_size=test_size)

def get_test_data():
    test_data = all_data[all_data["label"] == "test"].reset_index(drop=True)
    return test_data.drop(["label", "target"], axis=1)
```

---

## âš ï¸ å…­ã€å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆOutlier Detectionï¼‰

> â— åŸå› ï¼šå¼‚å¸¸æ ·æœ¬å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆæˆ–åå·®å¢å¤§ã€‚

### 6.1 ç®—æ³•æ€æƒ³ï¼šåŸºäºæ®‹å·® Z-score åˆ¤åˆ«

é€šè¿‡æ‹Ÿåˆä¸€ä¸ªå›å½’æ¨¡å‹ï¼Œè®¡ç®—çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„æ®‹å·®ï¼Œç”¨æ ‡å‡†åˆ†æ•° |Z| > Ïƒ æ¥è¯†åˆ«å¼‚å¸¸ç‚¹ã€‚

### 6.2 å®ç° `find_outliers` å‡½æ•°

```python
def find_outliers(model, X, y, sigma=3):
    model.fit(X, y)
    y_pred = pd.Series(model.predict(X), index=y.index)
    resid = y - y_pred
    mean_resid = resid.mean()
    std_resid = resid.std()
  
    Z = (resid - mean_resid) / std_resid
    outliers = Z[abs(Z) > sigma].index

    print(f"RÂ² = {model.score(X, y):.3f}")
    print(f"MSE = {mean_squared_error(y, y_pred):.3f}")
    print("-----------------------------------")
    print(f"Mean residual: {mean_resid:.3f}")
    print(f"Std residual: {std_resid:.3f}")
    print(f"Outliers count: {len(outliers)}")

    # ç»˜åˆ¶ä¸‰è”å›¾ï¼ˆçœŸå®å€¼ vs é¢„æµ‹å€¼ã€æ®‹å·®å›¾ã€æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾ï¼‰
    fig, axes = plt.subplots(1, 3, figsize=(15,5))
  
    axes[0].plot(y, y_pred, ".", label="Accepted")
    axes[0].plot(y.loc[outliers], y_pred.loc[outliers], "ro", label="Outlier")
    axes[0].set_xlabel("y"); axes[0].set_ylabel("y_pred")
    axes[0].legend()

    axes[1].plot(y, resid, ".", label="Accepted")
    axes[1].plot(y.loc[outliers], resid.loc[outliers], "ro", label="Outlier")
    axes[1].set_xlabel("y"); axes[1].set_ylabel("Residual")

    Z.plot.hist(bins=50, ax=axes[2], alpha=0.7)
    Z.loc[outliers].plot.hist(color='r', bins=50, ax=axes[2], alpha=0.7)
    axes[2].set_xlabel("Z-score")
    axes[2].legend(['Accepted', 'Outlier'])

    plt.savefig("./export_data/outliers.png")
    return outliers
```

### 6.3 å››ç§æ¨¡å‹æ£€æµ‹å¼‚å¸¸å€¼å¯¹æ¯”

| æ¨¡å‹ | ç‰¹ç‚¹ | æ˜¯å¦é€‚åˆå¼ºéçº¿æ€§ |
|------|------|----------------|
| Ridge | L2 æ­£åˆ™åŒ–ï¼Œç¨³å¥ | âœ… |
| Lasso | L1 æ­£åˆ™åŒ–ï¼Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹© | âœ… |
| SVR | åŸºäºæ ¸å‡½æ•°ï¼Œæ•æ‰éçº¿æ€§å…³ç³» | âœ…âœ… |
| XGBoost | é›†æˆæ ‘æ¨¡å‹ï¼Œé«˜è¡¨è¾¾èƒ½åŠ› | âœ…âœ…âœ… |

#### è¿è¡Œç»“æœæ±‡æ€»ï¼š

```python
outliers1 = find_outliers(Ridge(), X_train, y_train)     # â†’ 83 ä¸ª
outliers2 = find_outliers(Lasso(), X_train, y_train)     # â†’ 47 ä¸ª
outliers3 = find_outliers(SVR(), X_train, y_train)       # â†’ 152 ä¸ª
outliers4 = find_outliers(XGBRegressor(), X_train, y_train)  # â†’ 76 ä¸ª
```

> âœ… å¤šæ¨¡å‹äº¤å‰éªŒè¯å–å¹¶é›†æ›´å¯é ã€‚

### 6.4 åˆå¹¶å¼‚å¸¸å€¼å¹¶è¿‡æ»¤

```python
outliers12 = np.union1d(outliers1, outliers2)
outliers34 = np.union1d(outliers3, outliers4)
outliers_final = np.union1d(outliers12, outliers34)

print("Total outliers removed:", len(outliers_final))
all_data_drop = all_data.drop(labels=outliers_final)
print("New shape:", all_data_drop.shape)
```

ğŸ“‰ æœ€ç»ˆåˆ é™¤çº¦ **~200** è¡Œè®°å½•ï¼Œå‡è½»äº†å™ªå£°å½±å“ã€‚

âœ”ï¸ è¾“å‡ºå›¾åƒè·¯å¾„ï¼š`./export_data/outliers.png`

---

## ğŸ”— ä¸ƒã€å¤šé‡å…±çº¿æ€§åˆ†æï¼ˆMulticollinearityï¼‰

> âŒ é—®é¢˜æœ¬è´¨ï¼šä¸¤ä¸ªæˆ–å¤šä¸ªç‰¹å¾é«˜åº¦çº¿æ€§ç›¸å…³ â†’ è®¾è®¡çŸ©é˜µå¥‡å¼‚ â†’ çº¿æ€§å›å½’æ— æ³•æ±‚è§£ã€‚

### 7.1 ä¸ºä½•è¦å…³æ³¨ï¼Ÿ

- å¯¼è‡´æƒé‡ä¸ç¨³å®šï¼ˆå¾®å°å˜åŒ–å¼•å‘å·¨å¤§æ³¢åŠ¨ï¼‰
- é™ä½æ¨¡å‹è§£é‡Šæ€§
- å½±å“æ¢¯åº¦ä¸‹é™æ”¶æ•›é€Ÿåº¦

### 7.2 å¸¸è§æ£€æµ‹æ–¹æ³•

#### âœ… æ–¹æ³•ä¸€ï¼šç›¸å…³ç³»æ•°çŸ©é˜µçƒ­åŠ›å›¾

```python
corrmatrix = train_data.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corrmatrix, cmap="coolwarm", center=0, square=True)
plt.title("Feature Correlation Map")
plt.show()
```

ğŸ“Œ **å…³é”®å‘ç°**ï¼š
- è‹¥æŸä¸¤åˆ—ç›¸å…³ç³»æ•°ç»å¯¹å€¼ > 0.8ï¼Œåˆ™å¯èƒ½å­˜åœ¨å…±çº¿æ€§ï¼›
- å¯è§†åŒ–è¾…åŠ©åˆ¤æ–­å“ªäº›ç‰¹å¾ç°‡éœ€è¦åˆå¹¶æˆ–å‰”é™¤ã€‚

#### âœ… æ–¹æ³•äºŒï¼šæ–¹å·®è†¨èƒ€å› å­ï¼ˆVIFï¼‰

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print(vif_data.sort_values(by='VIF', ascending=False))
```

> âœ… `VIF > 5` æˆ– `10` å³è§†ä¸ºå­˜åœ¨ä¸¥é‡å…±çº¿æ€§ã€‚

### 7.3 è§£å†³æ–¹æ¡ˆ

| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | æ¨èæŒ‡æ•° |
|------|----------|---------|
| åˆ é™¤é«˜åº¦ç›¸å…³ç‰¹å¾ | å¿«é€Ÿæ¸…ç†å†—ä½™ | â­â­â­â­ |
| PCA é™ç»´ | å¤šç»´å¤æ‚å…³è” | â­â­â­â­â­ |
| Ridge/Lasso æ­£åˆ™åŒ– | å¼ºåŒ–é²æ£’æ€§ | â­â­â­â­â­ |

ğŸ”§ **å»ºè®®é¡ºåº**ï¼š
1. å…ˆåˆ å†—ä½™é¡¹ï¼ˆæ¯”å¦‚ `V0` å’Œ `V1` å…±çº¿æ€§å¼ºï¼‰
2. å†ç”¨æ­£åˆ™åŒ–æ¨¡å‹åº”å¯¹æ®‹ä½™å…±çº¿æ€§
3. å¦‚éœ€é™ç»´â†’é‡‡ç”¨ PCA / t-SNE

---

## ğŸ“Š æ€»ç»“ï¼šå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹å›¾

```mermaid
graph TD
    A[åŸå§‹æ•°æ®] --> B{æ˜¯å¦å«ç¼ºå¤±å€¼?}
    B -- æ˜¯ --> C[å¡«è¡¥/åˆ é™¤]
    B -- å¦ --> D[åˆ†ç¦»è®­ç»ƒé›† & æµ‹è¯•é›†]
    D --> E[å½’ä¸€åŒ–: MinMaxScaler]
    E --> F[æ­£æ€åŒ–: Yeo-Johnsonå˜æ¢]
    F --> G[å¼‚å¸¸å€¼æ£€æµ‹: å¤šæ¨¡å‹äº¤å‰éªŒè¯]
    G --> H[è¿‡æ»¤å¼‚å¸¸å€¼]
    H --> I[å…±çº¿æ€§åˆ†æ: VIF / çƒ­åŠ›å›¾]
    I --> J[ç‰¹å¾é€‰æ‹©/PCA/æ­£åˆ™åŒ–]
    J --> K[å‡†å¤‡é€å…¥æ¨¡å‹è®­ç»ƒ]
```

---

## ğŸ§  å­¦ä¹ æ”¶è·æ€»ç»“

| æŠ€èƒ½ç‚¹ | æŠ€æœ¯è¦ç‚¹ | åº”ç”¨å»ºè®® |
|--------|-----------|------------|
| æ•°æ®æ¸…æ´— | åˆ é™¤æ— ç”¨åˆ—ã€å¤„ç†æ— æ•ˆæ ‡ç­¾ | å»ºè®®ä½¿ç”¨ `.copy()` |
| å½’ä¸€åŒ– | MinMaxScaler ä¼˜äºæ‰‹åŠ¨å…¬å¼ | é€‚åˆç¥ç»ç½‘ç»œè¾“å…¥ |
| æ­£æ€åŒ– | Yeo-Johnson é€šç”¨æ€§å¼º | æ›¿ä»£ Box-Cox |
| å¼‚å¸¸å€¼æ£€æµ‹ | å¤šæ¨¡å‹èåˆï¼Œæé«˜å¬å›ç‡ | é¿å…å•ä¸€æ¨¡å‹æ¼æŠ¥ |
| å…±çº¿æ€§å¤„ç† | åˆ©ç”¨ VIF å’Œç›¸å…³æ€§çŸ©é˜µ | æ—©è¯†åˆ«æ—©å¹²é¢„ |

---

## ğŸ’¬ å†™åœ¨æœ€å

> â€œæ•°æ®è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™ã€‚â€
> ä¸€æ¬¡ç»†è‡´çš„æ•°æ®é¢„å¤„ç†ï¼Œå¾€å¾€æ¯”è°ƒå‚æ›´é‡è¦ã€‚

æœ¬ç¯‡ä»ç†è®ºåˆ°ä»£ç ã€ä»å¯è§†åŒ–åˆ°å®é™…è¾“å‡ºï¼Œå…¨é¢å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€æ¡**å¹²å‡€ã€é«˜æ•ˆã€ç¨³å¥çš„æ•°æ®é¢„å¤„ç†æµæ°´çº¿**ã€‚æ— è®ºä½ æ˜¯å‚åŠ ç«èµ›ï¼Œè¿˜æ˜¯å·¥ä¸šé¡¹ç›®ï¼Œè¿™äº›æŠ€å·§éƒ½å€¼å¾—æ”¶è—ï¼

ğŸ“ **é…å¥—èµ„æºå»ºè®®**ï¼š
- ä¿å­˜ç”Ÿæˆå›¾åƒè‡³ `/export_data/`
- å°†æ ¸å¿ƒå‡½æ•°æ”¾å…¥å·¥å…·è„šæœ¬ `preprocess.py`
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç®¡ç†æ•°æ®å¤„ç†æµç¨‹

---

ğŸ“Œ **å‚è€ƒæ–‡çŒ®**
1. Wikipedia: [Support Vector Regression](https://en.wikipedia.org/wiki/Support_vector_regression)
2. Scikit-learn å®˜æ–¹æ–‡æ¡£ï¼š[PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)
3. ISLR Chapter 6 â€“ Linear Models and Regularization

---

âœ¨ **æ¬¢è¿ç‚¹èµ + æ”¶è— + è½¬å‘**ï¼Œè®©æ›´å¤šçš„ AI å­¦ä¹ è€…çœ‹åˆ°è¿™ä»½å®ç”¨æ•™ç¨‹ï¼


